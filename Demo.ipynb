{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68ecf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_files.txt\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "parsed_data = [(int(line.split()[0]), line.split()[1]) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f756d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = None\n",
    "for i, (file, category) in enumerate(parsed_data):\n",
    "    if category == 'Switch' and file == 100911:\n",
    "        start_index = i\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6871053f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.\n",
      "Traceback (most recent call last):\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/file_io.py\", line 946, in __log_tmetry_keys\n",
      "    handler.log_event()\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/event_logger.py\", line 97, in log_event\n",
      "    del self._evt\n",
      "AttributeError: _evt\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "UserWarning: R is not a valid rotation matrix\n"
     ]
    }
   ],
   "source": [
    "if start_index is None:\n",
    "    print(\"Start index not found\")\n",
    "else:\n",
    "    # Process the data starting from the found index\n",
    "    io = IO()\n",
    "    for file, category in parsed_data[start_index:]:\n",
    "        xyz, rgb = normalize_pc(f\"data/{category}/{file}/pc.ply\", f'examples/zeroshot_{category}', io, device) #read Point cloud and rgb in the format n,3\n",
    "        img_dir, pc_idx, screen_coords = render_pc(xyz, rgb, f'examples/zeroshot_{category}', device, file)\n",
    "        \n",
    "        np.savez(f'examples/zeroshot_{category}/rendered_img_test/{file}/pcidx_screencoords.npz', \n",
    "                 pc_idx=pc_idx, \n",
    "                 screen_coords=screen_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02152bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = {}\n",
    "for category in categories:\n",
    "    filtered_data = [f\"data/{category}/{item[0]}\" for item in parsed_data if item[1] == category]\n",
    "    results[category] = filtered_data[:num_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3771cc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /rhome/kamburoglu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /rhome/kamburoglu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "final text_encoder_type: bert-base-uncased\n",
      "FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Suitcase inference DINO, GLIP ==> SAM started:\n",
      "------Rendering------\n",
      "An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.\n",
      "Traceback (most recent call last):\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/file_io.py\", line 946, in __log_tmetry_keys\n",
      "    handler.log_event()\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/event_logger.py\", line 97, in log_event\n",
      "    del self._evt\n",
      "AttributeError: _evt\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "------Rendering Completed------\n",
      "------Rendering------\n",
      "An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.\n",
      "Traceback (most recent call last):\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/file_io.py\", line 946, in __log_tmetry_keys\n",
      "    handler.log_event()\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/event_logger.py\", line 97, in log_event\n",
      "    del self._evt\n",
      "AttributeError: _evt\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "------Rendering Completed------\n",
      "------Rendering------\n",
      "An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.\n",
      "Traceback (most recent call last):\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/file_io.py\", line 946, in __log_tmetry_keys\n",
      "    handler.log_event()\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/event_logger.py\", line 97, in log_event\n",
      "    del self._evt\n",
      "AttributeError: _evt\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "------Rendering Completed------\n",
      "------Rendering------\n",
      "An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.\n",
      "Traceback (most recent call last):\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/file_io.py\", line 946, in __log_tmetry_keys\n",
      "    handler.log_event()\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/event_logger.py\", line 97, in log_event\n",
      "    del self._evt\n",
      "AttributeError: _evt\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "------Rendering Completed------\n",
      "------Rendering------\n",
      "An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.\n",
      "Traceback (most recent call last):\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/file_io.py\", line 946, in __log_tmetry_keys\n",
      "    handler.log_event()\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/event_logger.py\", line 97, in log_event\n",
      "    del self._evt\n",
      "AttributeError: _evt\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "------Rendering Completed------\n",
      "------Rendering------\n",
      "An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.\n",
      "Traceback (most recent call last):\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/file_io.py\", line 946, in __log_tmetry_keys\n",
      "    handler.log_event()\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/event_logger.py\", line 97, in log_event\n",
      "    del self._evt\n",
      "AttributeError: _evt\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "------Rendering Completed------\n",
      "------Rendering------\n",
      "An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.\n",
      "Traceback (most recent call last):\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/file_io.py\", line 946, in __log_tmetry_keys\n",
      "    handler.log_event()\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/event_logger.py\", line 97, in log_event\n",
      "    del self._evt\n",
      "AttributeError: _evt\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "------Rendering Completed------\n",
      "------Rendering------\n",
      "An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.\n",
      "Traceback (most recent call last):\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/file_io.py\", line 946, in __log_tmetry_keys\n",
      "    handler.log_event()\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/event_logger.py\", line 97, in log_event\n",
      "    del self._evt\n",
      "AttributeError: _evt\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "------Rendering Completed------\n",
      "------Rendering------\n",
      "An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.\n",
      "Traceback (most recent call last):\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/file_io.py\", line 946, in __log_tmetry_keys\n",
      "    handler.log_event()\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/event_logger.py\", line 97, in log_event\n",
      "    del self._evt\n",
      "AttributeError: _evt\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python testIOU.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23bc67d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../GroundingDINO')\n",
    "sys.path.insert(0,'../SAM')\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from pytorch3d.io import IO\n",
    "import numpy as np\n",
    "from src.utils import normalize_pc,save_colored_pc\n",
    "from src.render_pc import render_pc\n",
    "from src.gen_superpoint import gen_superpoint\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "from torchvision.ops import nms\n",
    "import json\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import distinctipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30d54d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolobbox2bbox(yolobox):\n",
    "    x = yolobox[:,0]\n",
    "    y = yolobox[:,1]\n",
    "    w = yolobox[:,2]\n",
    "    h = yolobox[:,3]\n",
    "    xyxy = np.zeros_like(yolobox)\n",
    "    xyxy[:,0] = x-w/2\n",
    "    xyxy[:,1] = y-h/2\n",
    "    xyxy[:,2] = x+w/2\n",
    "    xyxy[:,3] = y+h/2\n",
    "    return xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88597d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pc_within_bbox(x1, y1, x2, y2, pc):  \n",
    "    flag = np.logical_and(pc[:, 0] > x1, pc[:, 0] < x2)\n",
    "    flag = np.logical_and(flag, pc[:, 1] > y1)\n",
    "    flag = np.logical_and(flag, pc[:, 1] < y2)\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1786d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDinoPrompt(metaData,className):\n",
    "    listOfParts = metaData[className]\n",
    "    prompt = \"\"\n",
    "    partList = {}\n",
    "    for i,part in enumerate(listOfParts):\n",
    "        prompt += f\"{className} {part}.\".lower()\n",
    "        partList[f\"{className} {part}\".lower()] = i\n",
    "    return prompt,partList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca9ca65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InferDINOSAMZeroShot(input_pc_file, category, modelDINO, predictorSAM, metaData, device,BOX_TRESHOLD = 0.2,\n",
    "    TEXT_TRESHOLD = 0.3, SCORE_THRESHOLD=0.2, n_neighbors = 21, n_pass=3, save_dir=\"tmp\"):\n",
    "    \n",
    "#     print(\"-----Zero-shot inference of %s-----\" % input_pc_file)\n",
    "    TEXT_PROMPT,partList = toDinoPrompt(metaData, category)\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(f\"{save_dir}/rendered_img\", exist_ok=True) #create the necessary save directories\n",
    "    os.makedirs(f\"{save_dir}/dino_pred\", exist_ok=True)\n",
    "    os.makedirs(f\"{save_dir}/semantic_segDino_KNN\", exist_ok=True)\n",
    "    \n",
    "    io = IO()\n",
    "    xyz, rgb = normalize_pc(input_pc_file, save_dir, io, device) #read Point cloud and rgb in the format n,3\n",
    "    img_dir, pc_idx, screen_coords = render_pc(xyz, rgb, save_dir, device) #create the rendered 2D images and return \n",
    "    # pc_idx = hxw where every pixel has a PC index correspondence   \n",
    "    preds = []\n",
    "    for i in range(pc_idx.shape[0]):\n",
    "        image_source, image = load_image(f\"{save_dir}/rendered_img/{i}.png\") #load rgb images\n",
    "        predictorSAM.set_image(image_source)\n",
    "#         print(\"[dino inference...]\")\n",
    "        boxes, logits, phrases = predict(\n",
    "                                        model=modelDINO,\n",
    "                                        image=image,\n",
    "                                        caption=TEXT_PROMPT,\n",
    "                                        box_threshold=BOX_TRESHOLD,\n",
    "                                        text_threshold=TEXT_TRESHOLD\n",
    "                                    )\n",
    "        phrases = np.array(phrases) #just to fix indexing\n",
    "\n",
    "        xyxy = yolobbox2bbox(boxes)*image.shape[-1] #change bbox format to xyxy and scale with image size\n",
    "        \n",
    "        nms_mask = []\n",
    "        for t,bbox in enumerate(xyxy): \n",
    "            if check_pc_within_bbox(bbox[0], bbox[1], bbox[2], bbox[3], screen_coords[i]).mean() < 0.95: \n",
    "                nms_mask.append(t)\n",
    "        xyxy = xyxy[nms_mask]\n",
    "        boxes = boxes[nms_mask]\n",
    "        logits = logits[nms_mask]\n",
    "        phrases = phrases[nms_mask]\n",
    "        \n",
    "        \n",
    "        \n",
    "        nms_indexes = nms(torch.tensor(xyxy) , logits, 0.5).numpy() #non maximum supression\n",
    "\n",
    "        nms_mask = []\n",
    "        for t,index in enumerate(nms_indexes):\n",
    "            if phrases[index].lower() in partList.keys():\n",
    "                nms_mask.append(t)\n",
    "        nms_indexes = nms_indexes[nms_mask] #this is a temporary fix for DINO returning different classes that are not in the PROMPT\n",
    "        # another fix is needed for this as this eleminates some important segments such as chair back as the phrase is not exact to PROMPT\n",
    "\n",
    "        input_boxes = torch.tensor(xyxy[nms_indexes], device=predictorSAM.device)    \n",
    "        transformed_boxes = predictorSAM.transform.apply_boxes_torch(input_boxes, image_source.shape[:2])\n",
    "\n",
    "        if(transformed_boxes.numel() == 0):\n",
    "            transformed_boxes = None\n",
    "            \n",
    "        masks, _, _ = predictorSAM.predict_torch(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            boxes=transformed_boxes,\n",
    "            multimask_output=False,\n",
    "        )    #create segmentation masks with sam\n",
    "\n",
    "        for index,j in enumerate(nms_indexes):\n",
    "            preds.append({'image_id': i, 'category_id': phrases[j], \n",
    "                          'bbox': boxes[j]*image.shape[-1], \n",
    "                          'score': logits[j],\n",
    "                          'mask':masks[index,0]   \n",
    "                         }\n",
    "                        )\n",
    "        annotated_frame = annotate(image_source=image_source, boxes=boxes[nms_indexes], logits=logits[nms_indexes], phrases=phrases[nms_indexes])\n",
    "        cv2.imwrite(f\"{save_dir}/dino_pred/{i}.png\", annotated_frame) #save an annotated image for DINO debugging\n",
    "        \n",
    "    pc_aggMask = torch.zeros((xyz.shape[0],len(partList)+1)) #this is a segment agg mask we sum all the scores from our bboxes \n",
    "    #into their own respective channel, the last channel is for unsegmented parts\n",
    "    pc_aggMask[:,-1] = SCORE_THRESHOLD #we can set a confidence threshold by setting the unsegmented score\n",
    "    for prediction in preds:\n",
    "        maskedPC_idx = pc_idx[prediction[\"image_id\"],prediction[\"mask\"].cpu().numpy()] #this gives you the pc idx of the points that are inside the mask\n",
    "        index_pcMasked = np.unique(maskedPC_idx)[1:] # we only need the unique idx and the first id is always -1 meaning not found\n",
    "        pc_aggMask[index_pcMasked,partList[prediction[\"category_id\"]]] += prediction[\"score\"] #add up all the scores for each part\n",
    "    pc_seg_classes = torch.argmax(pc_aggMask,dim=-1) #select the highest score as our segmentation class\n",
    "    #if non of the part scores are over the SCORE_THRESHOLD it will be left unsegmented\n",
    "    partColors = distinctipy.get_colors(len(partList))\n",
    "    rgb_sem_merged = np.zeros((xyz.shape[0], 3))\n",
    "    # since projections are not exact meaning not every PC point is rendered into our image our backprojections are not dense\n",
    "    # use KNN to smooth these backprojections \n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors, algorithm='kd_tree').fit(xyz) #create a knn\n",
    "    \n",
    "    results = {\"partseg_rgbs\":{}}\n",
    "    for colorId,part in enumerate(partList):\n",
    "        pc_part_idx = np.zeros((xyz.shape[0]),dtype=int)\n",
    "        rgb_sem = np.zeros((xyz.shape[0],3))\n",
    "        pc_part_idx[torch.where(pc_seg_classes==partList[part])] = 1\n",
    "        \n",
    "        for pass_ in range(n_pass):\n",
    "            notColoredIndexes = torch.where(pc_seg_classes!=partList[part]) #find non segmented parts for smoothing\n",
    "\n",
    "            n_indexes = nn.kneighbors(xyz[notColoredIndexes],n_neighbors+1,return_distance=False)\n",
    "            n_indexes = n_indexes[:,1:] #get n_neighbors for the points, the first index is always the point itself so delete that\n",
    "            #we have dense point clouds so distance based measures are not necessary and sometimes give worst results\n",
    "            flag = pc_part_idx[n_indexes].mean(axis=1) \n",
    "            \n",
    "            flag[np.where(flag>=0.4)] = 1 #and segmnent the points where the mean of neighbours are colored %40 or over\n",
    "            flag[np.where(flag<0.4)] = 0\n",
    "            pc_part_idx[notColoredIndexes] = flag\n",
    "           \n",
    "        rgb_sem[pc_part_idx.astype(bool)] = partColors[colorId]\n",
    "        rgb_sem_merged += rgb_sem\n",
    "        save_colored_pc(f\"{save_dir}/semantic_segDino_KNN/{part}.ply\", xyz, rgb_sem)\n",
    "    \n",
    "        results[\"partseg_rgbs\"][part] = rgb_sem\n",
    "        \n",
    "    save_colored_pc(f\"{save_dir}/semantic_segDino_KNN/{category}.ply\", xyz, rgb_sem_merged)\n",
    "    results[\"partList\"] = partList\n",
    "    results[\"xyz\"] = xyz\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f89a8552",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a157ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "modelDINO = load_model(\"../GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py\",\n",
    "                       \"../GroundingDINO/weights/groundingdino_swinb_cogcoor.pth\",\n",
    "                      device=device\n",
    "                      )\n",
    "\n",
    "sam_checkpoint = \"../SAM/weights/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=\"cuda\")\n",
    "\n",
    "predictorSAM = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0d04dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metaData = json.load(open(\"./PartNetE_meta.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "601da3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"Suitcase\"\n",
    "input_pc_file = f\"data/{category}//.ply\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b37a83b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"Chair\"\n",
    "input_pc_file = f\"data/{category}/3091\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae3bc745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /rhome/kamburoglu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /rhome/kamburoglu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from pytorch3d.io import IO\n",
    "import numpy as np\n",
    "from src.utils import normalize_pc\n",
    "from src.render_pc import render_pc\n",
    "from src.glip_inference import glip_inference, load_model\n",
    "from src.gen_superpoint import gen_superpoint\n",
    "from src.bbox2seg import bbox2seg\n",
    "\n",
    "def Infer(input_pc_file, category, part_names,glip_demo, save_dir=\"tmp\"):       \n",
    "#     print(\"-----Zero-shot inference of %s-----\" % input_pc_file)\n",
    "\n",
    "#     print(\"[creating tmp dir...]\")\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        torch.cuda.set_device(device)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    io = IO()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "#     print(\"[normalizing input point cloud...]\")\n",
    "    xyz, rgb = normalize_pc(input_pc_file, save_dir, io, device)\n",
    "    \n",
    "#     print(\"[rendering input point cloud...]\")\n",
    "    img_dir, pc_idx, screen_coords = render_pc(xyz, rgb, save_dir, device)\n",
    "    \n",
    "#     print(\"[glip infrence...]\")\n",
    "    preds = glip_inference(glip_demo, save_dir, part_names)\n",
    "    \n",
    "#     print('[generating superpoints...]')\n",
    "    superpoint = gen_superpoint(xyz, rgb, visualize=True, save_dir=save_dir)\n",
    "    \n",
    "#     print('[converting bbox to 3D segmentation...]')\n",
    "    sem_seg, ins_seg = bbox2seg(xyz, superpoint, preds, screen_coords, pc_idx, part_names, save_dir, solve_instance_seg=False)\n",
    "    \n",
    "#     print(\"[finish!]\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "982aa3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo import Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86891d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.glip_inference import load_model_glip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21cd9d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['bert.pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "config =\"GLIP/configs/glip_Swin_L.yaml\"\n",
    "weight_path = \"models/glip_large_model.pth\"\n",
    "#     print(\"[loading GLIP model...]\")\n",
    "glip_demo = load_model_glip(config, weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8682d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mIOU(label, predictions, case='dino', verbose=True):\n",
    "    \"\"\"\n",
    "    Calculate mean IoU for semantic segmentation predictions.\n",
    "\n",
    "    Args:\n",
    "        label: Ground truth label containing 'semantic_seg'.\n",
    "        predictions: Predictions dictionary from the model.\n",
    "        case: Specify 'dino' or 'glip' for the respective models.\n",
    "        verbose: If True, prints detailed IoU calculation logs.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (list of IoUs, mean IoU).\n",
    "    \"\"\"\n",
    "    def log(msg):\n",
    "        if verbose:\n",
    "            print(msg)\n",
    "\n",
    "    try:\n",
    "        # Validate ground truth (label)\n",
    "        if 'semantic_seg' not in label.item():\n",
    "            raise ValueError(\"The label does not contain 'semantic_seg'.\")\n",
    "        semantic_seg = np.array(label.item()['semantic_seg'])\n",
    "        unique_parts = np.unique(semantic_seg)\n",
    "        unique_parts = unique_parts[unique_parts != -1]  # Exclude background (-1)\n",
    "\n",
    "        ious = []\n",
    "\n",
    "        if case == 'dino':\n",
    "            # Validate predictions for 'dino' case\n",
    "            if 'partList' not in predictions or 'partseg_rgbs' not in predictions:\n",
    "                raise ValueError(\n",
    "                    \"Invalid 'predictions' format for 'dino'. Expected keys: 'partList' and 'partseg_rgbs'.\"\n",
    "                )\n",
    "            part_list = predictions['partList']\n",
    "            partseg_rgbs = predictions['partseg_rgbs']\n",
    "\n",
    "            for part, part_id in part_list.items():\n",
    "                if part_id not in unique_parts:\n",
    "                    log(f\"{part} not present in ground truth, skipping IoU calculation.\")\n",
    "                    continue\n",
    "\n",
    "                gt_mask = semantic_seg == part_id\n",
    "                pred_mask = np.any(partseg_rgbs[part] != [0., 0., 0.], axis=-1)\n",
    "\n",
    "                intersection = np.logical_and(gt_mask, pred_mask).sum()\n",
    "                union = np.logical_or(gt_mask, pred_mask).sum()\n",
    "\n",
    "                iou = intersection / union if union > 0 else 0\n",
    "                ious.append(iou)\n",
    "                log(f\"IoU for {part}: {iou:.4f}\")\n",
    "\n",
    "        elif case == 'glip':\n",
    "            # Validate predictions for 'other' case\n",
    "            if 'part_names_ordered' not in predictions or 'sem_seg' not in predictions:\n",
    "                raise ValueError(\n",
    "                    \"Invalid 'predictions' format for 'glip'. Expected keys: 'part_names_ordered' and 'sem_seg'.\"\n",
    "                )\n",
    "            part_names = predictions['part_names_ordered']\n",
    "            sem_seg_pred = np.array(predictions['sem_seg'])\n",
    "            category = str(predictions['category'])\n",
    "            \n",
    "            for idx, part_name in enumerate(part_names):\n",
    "                if idx not in unique_parts:\n",
    "                    log(f\"{category.lower()} {part_name} not present in ground truth, skipping IoU calculation.\")\n",
    "                    continue\n",
    "\n",
    "                gt_mask = semantic_seg == idx\n",
    "                pred_mask = sem_seg_pred == idx\n",
    "\n",
    "                intersection = np.logical_and(gt_mask, pred_mask).sum()\n",
    "                union = np.logical_or(gt_mask, pred_mask).sum()\n",
    "\n",
    "                iou = intersection / union if union > 0 else 0\n",
    "                ious.append(iou)\n",
    "                log(f\"IoU for {part_name}: {iou:.4f}\")\n",
    "\n",
    "        else:\n",
    "            # Raise an error for unsupported cases\n",
    "            raise ValueError(f\"Unsupported case: {case}. Use 'dino' or 'glip'.\")\n",
    "\n",
    "        # Calculate mean IoU\n",
    "        if ious:\n",
    "            mean_iou = np.mean(ious)\n",
    "            log(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "        else:\n",
    "            log(\"No valid parts found in the ground truth. Mean IoU cannot be calculated.\")\n",
    "            mean_iou = None\n",
    "\n",
    "        return ious, mean_iou\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error calculating IoU: {e}\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16f07d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chair arm not present in ground truth, skipping IoU calculation.\n",
      "IoU for chair back: 0.4332\n",
      "IoU for chair leg: 0.8284\n",
      "IoU for chair seat: 0.2993\n",
      "IoU for chair wheel: 0.8238\n",
      "Mean IoU: 0.5962\n",
      "chair arm not present in ground truth, skipping IoU calculation.\n",
      "IoU for back: 0.9348\n",
      "IoU for leg: 0.8080\n",
      "IoU for seat: 0.9265\n",
      "IoU for wheel: 0.8174\n",
      "Mean IoU: 0.8717\n"
     ]
    }
   ],
   "source": [
    "ious_dino, mean_iou_dino = mIOU(label, predictions=preds, case='dino', verbose=True)\n",
    "ious_glip, mean_iou_glip = mIOU(label, predictions=anan, case='glip', verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8a3d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mIOU_dino_f(label, preds=None, predictions=None, case='dino', verbose=True):\n",
    "    \"\"\"\n",
    "    Calculate mean IoU for semantic segmentation predictions.\n",
    "\n",
    "    Args:\n",
    "        label: Ground truth label containing 'semantic_seg'.\n",
    "        preds: Predictions from the first model (DINO).\n",
    "        predictions: Predictions from the second model.\n",
    "        case: Specify 'dino' for the first model or 'other' for the second model.\n",
    "        verbose: If True, prints detailed IoU calculation logs.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (list of IoUs, mean IoU).\n",
    "    \"\"\"\n",
    "    def log(msg):\n",
    "        if verbose:\n",
    "            print(msg)\n",
    "    try:\n",
    "        # Validate ground truth (label)\n",
    "        if 'semantic_seg' not in label.item():\n",
    "            raise ValueError(\"The label does not contain 'semantic_seg'.\")\n",
    "        semantic_seg = np.array(label.item()['semantic_seg'])\n",
    "        unique_parts = np.unique(semantic_seg)\n",
    "        unique_parts = unique_parts[unique_parts != -1]  # Exclude background (-1)\n",
    "\n",
    "        ious = []\n",
    "\n",
    "        if case == 'dino':\n",
    "            # Validate 'dino' format\n",
    "            if not preds or 'partList' not in preds or 'partseg_rgbs' not in preds:\n",
    "                raise ValueError(\n",
    "                    \"Invalid 'preds' format for 'dino' case. Expected keys: 'partList' and 'partseg_rgbs'.\"\n",
    "                )\n",
    "            partList = preds['partList']\n",
    "            pred_partseg_rgbs = preds[\"partseg_rgbs\"]\n",
    "\n",
    "            for part, part_id in partList.items():\n",
    "                if part_id not in unique_parts:\n",
    "                    log(f\"{part} not present in ground truth, skipping IoU calculation.\")\n",
    "                    continue\n",
    "\n",
    "                gt_mask = semantic_seg == part_id\n",
    "                pred_mask = np.any(pred_partseg_rgbs[part] != [0., 0., 0.], axis=-1)\n",
    "\n",
    "                intersection = np.logical_and(gt_mask, pred_mask).sum()\n",
    "                union = np.logical_or(gt_mask, pred_mask).sum()\n",
    "\n",
    "                iou = intersection / union if union > 0 else 0\n",
    "                ious.append(iou)\n",
    "                log(f\"IoU for {part}: {iou:.4f}\")\n",
    "\n",
    "        elif case == 'other':\n",
    "            # Validate 'other' format\n",
    "            if not predictions or 'part_names_ordered' not in predictions or 'sem_seg' not in predictions:\n",
    "                raise ValueError(\n",
    "                    \"Invalid 'predictions' format for 'other' case. Expected keys: 'part_names_ordered' and 'sem_seg'.\"\n",
    "                )\n",
    "            part_names_ordered = predictions['part_names_ordered']\n",
    "            sem_seg_pred = np.array(predictions['sem_seg'])\n",
    "            category = str(predictions['category'])\n",
    "\n",
    "            for idx, part_name in enumerate(part_names_ordered):\n",
    "                if idx not in unique_parts:\n",
    "                    log(f\"{category.lower()} {part_name} not present in ground truth, skipping IoU calculation.\")\n",
    "                    continue\n",
    "\n",
    "                gt_mask = semantic_seg == idx\n",
    "                pred_mask = sem_seg_pred == idx\n",
    "\n",
    "                intersection = np.logical_and(gt_mask, pred_mask).sum()\n",
    "                union = np.logical_or(gt_mask, pred_mask).sum()\n",
    "\n",
    "                iou = intersection / union if union > 0 else 0\n",
    "                ious.append(iou)\n",
    "                log(f\"IoU for {part_name}: {iou:.4f}\")\n",
    "\n",
    "        else:\n",
    "            # Raise an error for unsupported cases\n",
    "            raise ValueError(f\"Unsupported case: {case}. Use 'dino' or 'other'.\")\n",
    "\n",
    "        # Calculate mean IoU\n",
    "        if ious:\n",
    "            mean_iou = np.mean(ious)\n",
    "            log(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "        else:\n",
    "            log(\"No valid parts found in the ground truth. Mean IoU cannot be calculated.\")\n",
    "            mean_iou = None\n",
    "\n",
    "        return ious, mean_iou\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error calculating IoU: {e}\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "edb4a735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chair arm not present in ground truth, skipping IoU calculation.\n",
      "IoU for chair back: 0.4332\n",
      "IoU for chair leg: 0.8284\n",
      "IoU for chair seat: 0.2993\n",
      "IoU for chair wheel: 0.8238\n",
      "Mean IoU: 0.5962\n",
      "Chair arm not present in ground truth, skipping IoU calculation.\n",
      "IoU for back: 0.9348\n",
      "IoU for leg: 0.8080\n",
      "IoU for seat: 0.9265\n",
      "IoU for wheel: 0.8174\n",
      "Mean IoU: 0.8717\n"
     ]
    }
   ],
   "source": [
    "# First case\n",
    "ious_dino, mean_iou_dino = mIOU_dino_f(label, preds=preds, case='dino', verbose=True)\n",
    "\n",
    "# Second case\n",
    "ious_other, mean_iou_other = mIOU_dino_f(label, predictions=anan, case='other', verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aff22f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arm not present in ground truth, skipping IoU calculation.\n",
      "IoU for back: 0.9524\n",
      "IoU for leg: 0.8116\n",
      "IoU for seat: 0.9449\n",
      "IoU for wheel: 0.8292\n",
      "Mean IoU: 0.8845\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract ground truth and predictions for the second model\n",
    "semantic_seg = np.array(label.item()['semantic_seg'])  # Ground truth\n",
    "unique_parts = np.unique(semantic_seg)  # Unique parts in the ground truth\n",
    "unique_parts = unique_parts[unique_parts != -1]  # Exclude -1\n",
    "sem_seg_pred = np.array(anan['sem_seg'])  # Predictions from the second model\n",
    "part_names_ordered = anan['part_names_ordered']  # Part names\n",
    "\n",
    "# Initialize IoU accumulator\n",
    "ious = []\n",
    "\n",
    "for idx, part_name in enumerate(part_names_ordered):\n",
    "    # Check if the part is in the ground truth\n",
    "    if idx not in unique_parts:\n",
    "        print(f\"{part_name} not present in ground truth, skipping IoU calculation.\")\n",
    "        continue\n",
    "\n",
    "    # Get the mask for the ground truth\n",
    "    gt_mask = semantic_seg == idx\n",
    "\n",
    "    # Get the mask for the predictions\n",
    "    pred_mask = sem_seg_pred == idx\n",
    "\n",
    "    # Calculate intersection and union\n",
    "    intersection = np.logical_and(gt_mask, pred_mask).sum()\n",
    "    union = np.logical_or(gt_mask, pred_mask).sum()\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = intersection / union if union > 0 else 0\n",
    "    ious.append(iou)\n",
    "\n",
    "    print(f\"IoU for {part_name}: {iou:.4f}\")\n",
    "\n",
    "# Calculate mean IoU only if there are valid IoUs\n",
    "if ious:\n",
    "    mean_iou = np.mean(ious)\n",
    "    print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "else:\n",
    "    print(\"No valid parts found in the ground truth. Mean IoU cannot be calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea492320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from demoDINO import InferDINOSAMZeroShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18c4f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDinoPrompt(metaData,className):\n",
    "    listOfParts = metaData[className]\n",
    "    prompt = \"\"\n",
    "    partList = {}\n",
    "    for i,part in enumerate(listOfParts):\n",
    "        prompt += f\"{className} {part}.\".lower()\n",
    "        partList[f\"{className} {part}\".lower()] = i\n",
    "    return prompt,partList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "530974ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_PROMPT,partList = toDinoPrompt(metaData, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5047da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.\n",
      "Traceback (most recent call last):\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/file_io.py\", line 946, in __log_tmetry_keys\n",
      "    handler.log_event()\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/event_logger.py\", line 97, in log_event\n",
      "    del self._evt\n",
      "AttributeError: _evt\n",
      "UserWarning: R is not a valid rotation matrix\n"
     ]
    }
   ],
   "source": [
    "io = IO()\n",
    "xyz, rgb = normalize_pc(input_pc_file + \"/pc.ply\", f'examples/zeroshot_{category}', io, device) #read Point cloud and rgb in the format n,3\n",
    "img_dir, pc_idx, screen_coords = render_pc(xyz, rgb,  f'examples/zeroshot_{category}', device, \"179\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2fedd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: R is not a valid rotation matrix\n",
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n"
     ]
    }
   ],
   "source": [
    "glip_pred = Infer(xyz, rgb, screen_coords, pc_idx, category, metaData[category],glip_demo,device, save_dir=\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e957edcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GroundingDINO(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): DeformableTransformerEncoderLayer(\n",
       "          (self_attn): MultiScaleDeformableAttention(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): DeformableTransformerEncoderLayer(\n",
       "          (self_attn): MultiScaleDeformableAttention(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): DeformableTransformerEncoderLayer(\n",
       "          (self_attn): MultiScaleDeformableAttention(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): DeformableTransformerEncoderLayer(\n",
       "          (self_attn): MultiScaleDeformableAttention(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): DeformableTransformerEncoderLayer(\n",
       "          (self_attn): MultiScaleDeformableAttention(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): DeformableTransformerEncoderLayer(\n",
       "          (self_attn): MultiScaleDeformableAttention(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (text_layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fusion_layers): ModuleList(\n",
       "        (0): BiAttentionBlock(\n",
       "          (layer_norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (layer_norm_l): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): BiMultiHeadAttention(\n",
       "            (v_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (l_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (values_v_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (values_l_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (out_v_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (out_l_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "        (1): BiAttentionBlock(\n",
       "          (layer_norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (layer_norm_l): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): BiMultiHeadAttention(\n",
       "            (v_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (l_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (values_v_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (values_l_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (out_v_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (out_l_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "        (2): BiAttentionBlock(\n",
       "          (layer_norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (layer_norm_l): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): BiMultiHeadAttention(\n",
       "            (v_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (l_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (values_v_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (values_l_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (out_v_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (out_l_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "        (3): BiAttentionBlock(\n",
       "          (layer_norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (layer_norm_l): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): BiMultiHeadAttention(\n",
       "            (v_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (l_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (values_v_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (values_l_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (out_v_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (out_l_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "        (4): BiAttentionBlock(\n",
       "          (layer_norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (layer_norm_l): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): BiMultiHeadAttention(\n",
       "            (v_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (l_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (values_v_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (values_l_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (out_v_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (out_l_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "        (5): BiAttentionBlock(\n",
       "          (layer_norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (layer_norm_l): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): BiMultiHeadAttention(\n",
       "            (v_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (l_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (values_v_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (values_l_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (out_v_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (out_l_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): DeformableTransformerDecoderLayer(\n",
       "          (cross_attn): MultiScaleDeformableAttention(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Identity()\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ca_text): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (catext_dropout): Identity()\n",
       "          (catext_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Identity()\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout3): Identity()\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout4): Identity()\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): DeformableTransformerDecoderLayer(\n",
       "          (cross_attn): MultiScaleDeformableAttention(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Identity()\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ca_text): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (catext_dropout): Identity()\n",
       "          (catext_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Identity()\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout3): Identity()\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout4): Identity()\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): DeformableTransformerDecoderLayer(\n",
       "          (cross_attn): MultiScaleDeformableAttention(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Identity()\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ca_text): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (catext_dropout): Identity()\n",
       "          (catext_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Identity()\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout3): Identity()\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout4): Identity()\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): DeformableTransformerDecoderLayer(\n",
       "          (cross_attn): MultiScaleDeformableAttention(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Identity()\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ca_text): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (catext_dropout): Identity()\n",
       "          (catext_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Identity()\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout3): Identity()\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout4): Identity()\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): DeformableTransformerDecoderLayer(\n",
       "          (cross_attn): MultiScaleDeformableAttention(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Identity()\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ca_text): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (catext_dropout): Identity()\n",
       "          (catext_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Identity()\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout3): Identity()\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout4): Identity()\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): DeformableTransformerDecoderLayer(\n",
       "          (cross_attn): MultiScaleDeformableAttention(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Identity()\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ca_text): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (catext_dropout): Identity()\n",
       "          (catext_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Identity()\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout3): Identity()\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout4): Identity()\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ref_point_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (bbox_embed): ModuleList(\n",
       "        (0): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (4): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (class_embed): ModuleList(\n",
       "        (0): ContrastiveEmbed()\n",
       "        (1): ContrastiveEmbed()\n",
       "        (2): ContrastiveEmbed()\n",
       "        (3): ContrastiveEmbed()\n",
       "        (4): ContrastiveEmbed()\n",
       "        (5): ContrastiveEmbed()\n",
       "      )\n",
       "    )\n",
       "    (tgt_embed): Embedding(900, 256)\n",
       "    (enc_output): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (enc_out_bbox_embed): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (enc_out_class_embed): ContrastiveEmbed()\n",
       "  )\n",
       "  (bert): BertModelWarper(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (feat_map): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (input_proj): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (backbone): Joiner(\n",
       "    (0): SwinTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.009)\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.017)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.026)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.035)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.043)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.052)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.061)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.070)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.078)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (6): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.087)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (7): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.096)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (8): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.104)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (9): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.113)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (10): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.122)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (11): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.130)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (12): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.139)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (13): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.148)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (14): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.157)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (15): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.165)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (16): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.174)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (17): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.183)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.191)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.200)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): PositionEmbeddingSineHW()\n",
       "  )\n",
       "  (bbox_embed): ModuleList(\n",
       "    (0): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (4): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (5): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (class_embed): ModuleList(\n",
       "    (0): ContrastiveEmbed()\n",
       "    (1): ContrastiveEmbed()\n",
       "    (2): ContrastiveEmbed()\n",
       "    (3): ContrastiveEmbed()\n",
       "    (4): ContrastiveEmbed()\n",
       "    (5): ContrastiveEmbed()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelDINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8936c952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chair'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a0c22bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n"
     ]
    }
   ],
   "source": [
    "preds = InferDINOSAMZeroShot(TEXT_PROMPT, partList, xyz, pc_idx, screen_coords, category, \n",
    "                                     modelDINO, predictorSAM, device, \n",
    "                                     BOX_TRESHOLD=0.2, TEXT_TRESHOLD=0.3, \n",
    "                                     SCORE_THRESHOLD=0.2, n_neighbors=21, \n",
    "                                     n_pass=5, save_dir=f'examples/zeroshot_{category}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "645c534a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226129, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[\"xyz\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbb1ca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.load(input_pc_file + \"/label.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e0fa719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'semantic_seg': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'instance_seg': array([0, 0, 0, ..., 0, 0, 0], dtype=int32)},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1fb02c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
